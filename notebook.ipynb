{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bc9f9084",
      "metadata": {
        "id": "bc9f9084"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import decode_image\n",
        "from torchvision import models\n",
        "import os\n",
        "import pandas as pd\n",
        "import torchvision.transforms.v2 as T\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7427a53d",
      "metadata": {
        "id": "7427a53d"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, f\"{self.img_labels.iloc[idx, 0]}.jpg\")\n",
        "        image = decode_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label\n",
        "    \n",
        "def transform_target(label):\n",
        "    brand_to_idx = {\n",
        "        \"adidas\": 0,\n",
        "        \"converse\": 1,\n",
        "        \"nike\": 2,\n",
        "    }\n",
        "    return brand_to_idx[label]\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.Resize(227),\n",
        "    T.CenterCrop(227),\n",
        "    T.ToDtype(torch.float32, scale=True),\n",
        "])\n",
        "\n",
        "training_data = CustomImageDataset(annotations_file=\"data/train/annotations.csv\", img_dir=\"data/train/images\", transform=transform,target_transform=transform_target)\n",
        "\n",
        "testing_data = CustomImageDataset(annotations_file=\"data/test/annotations.csv\", img_dir=\"data/test/images\", transform=transform, target_transform=transform_target)\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(testing_data, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fe8e6a7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe8e6a7b",
        "outputId": "e2b00960-96d1-4bcf-f209-f2305ab4e117"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /home/heathcliff/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 233M/233M [01:21<00:00, 2.99MB/s] \n"
          ]
        }
      ],
      "source": [
        "model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n",
        "\n",
        "model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f5106359",
      "metadata": {},
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7406799e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eb3eb329",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Dropout(p=0.5, inplace=False)\n",
              "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): Dropout(p=0.5, inplace=False)\n",
              "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (5): ReLU(inplace=True)\n",
              "  (6): Linear(in_features=4096, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.classifier.requires_grad_(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6fac0b04",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.weight True\n",
            "1.bias True\n",
            "4.weight True\n",
            "4.bias True\n",
            "6.weight True\n",
            "6.bias True\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.classifier.named_parameters():\n",
        "    print(name, param.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "GfrlwWUNGqed",
      "metadata": {
        "id": "GfrlwWUNGqed"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  model.to(\"cuda\")\n",
        "else:\n",
        "  model.to(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "YrI2po_xIDed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrI2po_xIDed",
        "outputId": "ace6010e-89c2-4838-903a-b53e163f66dd"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "epochs = 20\n",
        "optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1xovbMzbJSHf",
      "metadata": {
        "id": "1xovbMzbJSHf"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch():\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optim.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "Qw-hj6dKKdwX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Qw-hj6dKKdwX",
        "outputId": "e8d2ecc8-4963-462b-fbde-06e799cc1588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n",
            "LOSS train 0.0 valid 0.8818612098693848\n",
            "ACCURACY valid 0.6371681415929203\n",
            "EPOCH 2:\n",
            "LOSS train 0.0 valid 0.6990245878696442\n",
            "ACCURACY valid 0.7610619469026548\n",
            "EPOCH 3:\n",
            "LOSS train 0.0 valid 1.0554738119244576\n",
            "ACCURACY valid 0.6194690265486725\n",
            "EPOCH 4:\n",
            "LOSS train 0.0 valid 0.7677903324365616\n",
            "ACCURACY valid 0.7256637168141593\n",
            "EPOCH 5:\n",
            "LOSS train 0.0 valid 0.6266226321458817\n",
            "ACCURACY valid 0.7787610619469026\n",
            "EPOCH 6:\n",
            "LOSS train 0.0 valid 0.7162491828203201\n",
            "ACCURACY valid 0.7522123893805309\n",
            "EPOCH 7:\n",
            "LOSS train 0.0 valid 0.6060633957386017\n",
            "ACCURACY valid 0.8230088495575221\n",
            "EPOCH 8:\n",
            "LOSS train 0.0 valid 0.7711837887763977\n",
            "ACCURACY valid 0.8053097345132744\n",
            "EPOCH 9:\n",
            "LOSS train 0.0 valid 1.0648573189973831\n",
            "ACCURACY valid 0.7522123893805309\n",
            "EPOCH 10:\n",
            "LOSS train 0.0 valid 0.777723491191864\n",
            "ACCURACY valid 0.8141592920353983\n",
            "EPOCH 11:\n",
            "LOSS train 0.0 valid 0.990197092294693\n",
            "ACCURACY valid 0.7699115044247787\n",
            "EPOCH 12:\n",
            "LOSS train 0.0 valid 0.8498388528823853\n",
            "ACCURACY valid 0.7876106194690266\n",
            "EPOCH 13:\n",
            "LOSS train 0.0 valid 0.7655074894428253\n",
            "ACCURACY valid 0.831858407079646\n",
            "EPOCH 14:\n",
            "LOSS train 0.0 valid 1.5033090710639954\n",
            "ACCURACY valid 0.7433628318584071\n",
            "EPOCH 15:\n",
            "LOSS train 0.0 valid 0.7802931070327759\n",
            "ACCURACY valid 0.8230088495575221\n",
            "EPOCH 16:\n",
            "LOSS train 0.0 valid 0.8031513690948486\n",
            "ACCURACY valid 0.8141592920353983\n",
            "EPOCH 17:\n",
            "LOSS train 0.0 valid 0.9836422204971313\n",
            "ACCURACY valid 0.7787610619469026\n",
            "EPOCH 18:\n",
            "LOSS train 0.0 valid 1.0107716619968414\n",
            "ACCURACY valid 0.7787610619469026\n",
            "EPOCH 19:\n",
            "LOSS train 0.0 valid 1.054041013121605\n",
            "ACCURACY valid 0.7964601769911505\n",
            "EPOCH 20:\n",
            "LOSS train 0.0 valid 1.0655388534069061\n",
            "ACCURACY valid 0.7964601769911505\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "  print('EPOCH {}:'.format(epoch + 1))\n",
        "\n",
        "  # Make sure gradient tracking is on, and do a pass over the data\n",
        "  model.train(True)\n",
        "  avg_loss = train_one_epoch()\n",
        "\n",
        "\n",
        "  running_vloss = 0.0\n",
        "  # Set the model to evaluation mode, disabling dropout and using population\n",
        "  # statistics for batch normalization.\n",
        "  model.eval()\n",
        "\n",
        "  preds = []\n",
        "  true = []\n",
        "\n",
        "  # Disable gradient computation and reduce memory consumption.\n",
        "  with torch.no_grad():\n",
        "      for i, vdata in enumerate(test_dataloader):\n",
        "          vinputs, vlabels = vdata\n",
        "          voutputs = model(vinputs)\n",
        "          preds = preds + list(torch.max(voutputs, 1).indices.cpu().numpy())\n",
        "          true = true + list(vlabels.cpu().numpy())\n",
        "          vloss = loss_fn(voutputs, vlabels)\n",
        "          running_vloss += vloss.item()\n",
        "  acc = accuracy_score(true, preds)\n",
        "  avg_vloss = running_vloss / (i + 1)\n",
        "  print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "  print('ACCURACY valid {}'.format(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "r008bjAhKtCu",
      "metadata": {
        "id": "r008bjAhKtCu"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53640c42",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "model-deployment",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
